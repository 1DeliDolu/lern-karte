# 🧩 3.2.1 Die Qualität von Daten beschreiben [Seite: 164]

Die Qualität von Daten ist entscheidend für die **Zuverlässigkeit von Informationen** und den **Erfolg datenbasierter Entscheidungen** in Unternehmen. Schlechte Datenqualität führt zu fehlerhaften Analysen, erhöhtem Aufwand und wirtschaftlichen Verlusten. Daher müssen Daten systematisch nach festgelegten Kriterien bewertet und gepflegt werden.

---

## 🎯 Grundprinzip

**Regel:**

> „Aus schlechten Daten können keine guten Informationen gewonnen werden.“

Datenqualität beschreibt die **Eignung eines Datensatzes**, ein bestimmtes Informationsbedürfnis zu erfüllen. Sie hängt von **Vollständigkeit, Korrektheit, Konsistenz und Aktualität** der Daten ab.

---

## 📊 Bedeutung der Datenqualität

* **Wirtschaftlich:** Fehlerhafte oder doppelte Datensätze verursachen Kosten (z. B. fehlerhafte Lieferungen, doppelter Versand).
* **Wissenschaftlich / technisch:** Messdaten müssen präzise, störungsfrei und wiederholbar sein.
* **Sicherheitspolitisch:** Nachrichtendienste prüfen Daten mehrfach zur Vermeidung falscher Schlussfolgerungen.
* **Unternehmerisch:** Entscheidungen, Marktanalysen und Verhandlungen beruhen auf korrekten Informationen.

> Eine gute Datenqualität erhöht die **Effizienz, Sicherheit und Verlässlichkeit** von Geschäftsprozessen.

---

## 📏 Qualitätskriterien für Daten

| **Kriterium**                            | **Beschreibung**                                                  | **Beispiel**                                                                     |
| :--------------------------------------- | :---------------------------------------------------------------- | :------------------------------------------------------------------------------- |
| **Vollständigkeit (Completeness)**       | Alle Attribute eines Datensatzes sind vorhanden und gültig.       | Fehlt in 15 % der Datensätze das Geburtsdatum, beträgt die Vollständigkeit 85 %. |
| **Eindeutigkeit (Uniqueness)**           | Jeder Datensatz ist eindeutig und nicht doppelt vorhanden.        | Zwei fast identische Datensätze mit unterschiedlichem Geburtsdatum → Dublette.   |
| **Korrektheit (Correctness)**            | Werte stimmen mit der Realität überein.                           | Geburtsdatum „01.01.1900“ bei lebenden Personen → offensichtlich falsch.         |
| **Aktualität (Timeliness)**              | Daten spiegeln den aktuellen Zustand wider.                       | 23 von 100 Adressen veraltet → Aktualität 77 %.                                  |
| **Genauigkeit (Accuracy)**               | Werte weisen die geforderte Präzision auf.                        | Messwerte sollen 4 Nachkommastellen haben, 2 fehlerhafte → Genauigkeit 98 %.     |
| **Konsistenz (Consistency)**             | Keine logischen Widersprüche innerhalb oder zwischen Datensätzen. | Anzahl 20 × Preis 20 ≠ Gesamtbetrag 500 → inkonsistent.                          |
| **Redundanzfreiheit (Non Redundant)**    | Informationen kommen nicht mehrfach vor.                          | Kundennummer doppelt vorhanden → nicht redundanzfrei.                            |
| **Relevanz (Relevancy)**                 | Daten passen zum Informationsziel.                                | Umsätze eines anderen Quartals → irrelevant.                                     |
| **Einheitlichkeit (Uniformity)**         | Einheitliche Struktur und Formatierung gleicher Werte.            | Datumsangabe teils im Format „YYYY-MM-DD“, teils „DD.MM.YYYY“.                   |
| **Zuverlässigkeit (Reliability)**        | Herkunft und Qualität der Daten sind nachvollziehbar.             | 94 % der Daten stammen aus bekannten, geprüften Quellen.                         |
| **Verständlichkeit (Understandability)** | Daten sind klar interpretierbar.                                  | „iPhone 12 Pro“ verständlicher als Modellcode „A1305“.                           |
| **Zugänglichkeit (Accessibility)**       | Daten sind schnell und einfach verfügbar.                         | Sendungsdaten 24 h online abrufbar.                                              |

---

## 🔄 Prozess zur Verbesserung der Datenqualität

1. **Analyse (Data Profiling):**

   * Erkennen von Fehlern, Dubletten, fehlenden oder widersprüchlichen Werten.
   * Ziel: Schwachstellen und Muster in den Daten aufdecken.

2. **Bereinigung (Data Cleaning):**

   * Korrigieren oder Löschen fehlerhafter Daten.
   * Maßnahmen: Dubletten entfernen, fehlende Werte ergänzen, Formate vereinheitlichen.

3. **Monitoring (Data Monitoring):**

   * Regelmäßige Überwachung der Datenqualität über längere Zeiträume.
   * Ohne Monitoring sinkt die Qualität durch **Datenalterung** kontinuierlich.

> ⚙️ Für große Datenmengen (Big Data) werden diese Prozesse zunehmend **automatisiert**, um Aufwand zu reduzieren.

---

## ⏳ Datenalterung

* Daten verändern sich im Laufe der Zeit (z. B. Adress- oder Preisänderungen).
* Ohne regelmäßige Aktualisierung wird der Datenbestand ungenau und unzuverlässig.
* **Konsequenz:** Sinkende Informationsqualität und Fehlentscheidungen.

---

## 🧠 Fazit

* **Ziel:** Hohe Datenqualität als Grundlage für verlässliche Informationen.
* **Mittel:** Regelmäßige Analyse, Bereinigung und Kontrolle.
* **Ergebnis:** Stabile, aktuelle und eindeutige Datenbestände, die betriebliche Entscheidungen fundiert unterstützen.
