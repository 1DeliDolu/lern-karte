# 3.2 Daten aus heterogenen Quellen analysieren und aufbereiten [Seite: 223]

**Kernaussage:** Die Gesellschaft erzeugt rasant wachsende Datenmengen. **Big Data** wird durch die **5 V** charakterisiert (**Volume**, **Variety**, **Velocity**, **Veracity**, **Value**). Ziel ist es, diese Daten systematisch zu **beziehen**, **analysieren** und **aufzubereiten**, um sie für datengetriebene Lösungen nutzbar zu machen. 

## 3.2.1 Daten beziehen

**Begriff:** Eine **Datenquelle** ist der Ort, an dem Daten entstehen (**primär**) oder für weitere Nutzung gespeichert werden (**sekundär**). Projekte des **Maschinellen Lernens** benötigen umfangreiche, qualitativ hochwertige Daten. 

**Quellenarten und Beispiele**

* **Interne Daten:** Unternehmensdatenbanken, **Logdaten**, **Dokumentationen/Wissensdatenbanken**, **Sensor**-Daten (Kameras, Mikrofone etc.). 
* **Externe Daten:** **amtliche/kommerzielle Statistiken**, **Webcrawler**-Daten, **gekaufte Datensätze**. 

**Datenqualität – zentrale Merkmale**

* **Vollständigkeit**, **Korrektheit**, **Konsistenz**, **Redundanzfreiheit**, **Verfügbarkeit**, **Einheitlichkeit**. Diese Kriterien prüfen, ob Daten die Information korrekt abbilden und technisch wie rechtlich verwendbar sind. 

**Verarbeitungsgrad & Formate**

* **Strukturierte**, **semistrukturierte** (z. B. Webseiten, Fotos, E-Mails) und **unstrukturierte** Daten (Texte, Bilder, Audio/Video). Übergänge sind fließend.
* Häufige Formate: **Datenbankschemata**, **ODS** (Tabellenkalkulation), **XML**, **JSON**, **INI**, **CSV**. 

**Beispiel (Klimaüberwachung „Yachthafen Resort“)**
Interne **Sensor**-Daten werden mit externen **Wetterdienst**-Daten kombiniert; Nutzungsrechte des Wetterdienstes klären, **DSGVO** entfällt hier, da keine personenbezogenen Daten verarbeitet werden. 

---

## 3.2.2 Daten analysieren

**Ziel:** Bestehende Datenmengen auf **Kenngrößen** und **Zusammenhänge** untersuchen; Visualisierung, Statistik oder Suche liefern erste Erkenntnisse. 

**Methoden**

* **Visualisierung** zur Beurteilung von Verläufen und Auffälligkeiten; z. B. Linien-/Balken-/Streudiagramme. 
* **Statistische Kenngrößen:** **Minimum/Maximum**, **Mittelwert**, **Median**, **Varianz**, **Standardabweichung**, **Entropie**. 
* **CRISP-DM** als Standardprozess: **Business Understanding**, **Data Understanding**, **Data Preparation**, **Modeling**, **Evaluation**, **Deployment**. 

**Werkzeuge**

* **Jupyter Notebook** (Analyse & Dokumentation), **pandas** (Import/Analyse), **Matplotlib** (Visualisierung). 

**Beispiel (Wetterdaten)**
CSV einlesen, Zeitraum filtern, **Ausreißer entfernen**, Kenngrößen berechnen (**Mittelwert**, **Median**, **Varianz**, **Standardabweichung**), **Entropie** bestimmen. 

---

## 3.2.3 Daten aufbereiten

**Schritte der Datenaufbereitung**

* **Codierung:** Daten digitalisieren/Zeichensatz vereinheitlichen. 
* **Vereinheitlichung:** Formate/Strukturen angleichen. 
* **Reduzierung:** Datenmenge auf Wesentliches verkleinern (z. B. **Graustufen**, **Hashing**). 
* **Konvertierung:** Problemadäquate Zielformate erzeugen. 
* **Klassifizierung:** Zuordnung in Kategorien als Vorbereitung für ML-Verfahren. 

**Werkzeuge (Auswahl)**

* **Anaconda** (Distribution inkl. Jupyter, **pandas**, **Matplotlib**), **NumPy**, **SciPy**, **Beautiful Soup**, **NLTK**, **OpenCV**. 

**Spezifika nach Datentyp**

* **Texte:** Zeichensatz vereinheitlichen, **Strukturangaben** bereinigen, **Tokenisierung**, **Lemmatisierung**, **Kleinschreibung**, **Stoppwörter** entfernen, **Bi-/Trigramme** bilden. 
* **Bilder:** Einheitliche **Formate/Größen/Farbtiefen**, Größenreduktion, ggf. **Graustufen** und **Ausschnittwahl**. 

**Beispiel (Chatbot „Yachthafen Resort“)**
Website-Inhalte werden extrahiert (**Beautiful Soup**) und sprachlich vorverarbeitet (**NLTK**: Tokenisierung, Lemmatisierung, Stoppwörter etc.), um Angebotsinformationen für einen Chatbot aufzubereiten. 

---

## Merksätze

* **Gute Daten schlagen komplexe Modelle:** **Datenqualität** und **Aufbereitung** sind erfolgskritisch. 
* **Strukturiertes Vorgehen** mit **CRISP-DM** sichert Nachvollziehbarkeit und Wiederholbarkeit. 
* **Passende Tools** (z. B. **Jupyter**, **pandas**, **Matplotlib**, **Anaconda**, **NumPy**, **SciPy**, **Beautiful Soup**, **NLTK**, **OpenCV**) beschleunigen Analyse und Aufbereitung.
